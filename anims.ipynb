{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5205e2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83eebb3c933a4a9f963d4ac7407a3793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | output: false\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Set the default style for matplotlib plots\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "# Load the Qwen3 model and tokenizer\n",
    "qwen3_model = (\n",
    "    AutoModel.from_pretrained(\"Qwen/Qwen3-Embedding-4B\", torch_dtype=torch.bfloat16)\n",
    "    .cuda()\n",
    "    .eval()\n",
    ")\n",
    "qwen3_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Embedding-4B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "042aafb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query text: ['Which', ' flora', ' is', ' found', ' in', ' Yosemite', '?', '<|endoftext|>']\n",
      "Text text: ['Black', ' bears', ' roam', ' the', ' area', ',', ' while', ' wild', 'flowers', ' and', ' towering', ' con', 'ifers', ' thrive', '.', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "query = \"Which flora is found in Yosemite?\"\n",
    "text = \"Black bears roam the area, while wildflowers and towering conifers thrive.\"\n",
    "# Tokenize the input text and query\n",
    "query_tokenized = qwen3_tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
    "text_tokenized = qwen3_tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Convert back to text\n",
    "tokenized_query = [qwen3_tokenizer.decode(t) for t in query_tokenized[\"input_ids\"][0]]\n",
    "tokenized_document = [qwen3_tokenizer.decode(t) for t in text_tokenized[\"input_ids\"][0]]\n",
    "print(\"Query text:\", tokenized_query)\n",
    "print(\"Text text:\", tokenized_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3817712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute embeddings for the query and text\n",
    "with torch.inference_mode():\n",
    "    query_embedding = (\n",
    "        qwen3_model(**query_tokenized).last_hidden_state.squeeze().detach().cpu()\n",
    "    )\n",
    "    text_embedding = (\n",
    "        qwen3_model(**text_tokenized).last_hidden_state.squeeze().detach().cpu()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20fa925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "import pickle\n",
    "\n",
    "# Save the embeddings to a file\n",
    "with open(\"assets/project_video_scene_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"query_embedding\": query_embedding,\n",
    "            \"document_embedding\": text_embedding,\n",
    "            \"tokenized_query\": tokenized_query,\n",
    "            \"tokenized_document\": tokenized_document,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b6a29",
   "metadata": {},
   "source": [
    "## Main animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f7de5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "from manim import *  # noqa: F403\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "\n",
    "class ProjectVideo(MovingCameraScene):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.shown_embeddings = 6\n",
    "\n",
    "    def get_color_from_cmap(self, value, cmap_name=\"hot\", cmap_range=(0, 1)):\n",
    "        cmap = cm.get_cmap(cmap_name)\n",
    "        norm_value = mcolors.Normalize(vmin=cmap_range[0], vmax=cmap_range[1])(value)\n",
    "        rgba = cmap(norm_value)\n",
    "        return ManimColor(rgba[:3])  # Only use RGB (ignore A)\n",
    "\n",
    "    def embed_text(\n",
    "        self,\n",
    "        text,\n",
    "        tokenized_text,\n",
    "        embedding,\n",
    "        move_camera,\n",
    "        title,\n",
    "        font_sizes=(54, 36, 20),\n",
    "        speed=1.0,\n",
    "    ):\n",
    "        title_tex = MarkupText(\n",
    "            title,\n",
    "            font_size=36,\n",
    "            color=WHITE,\n",
    "        )\n",
    "        title_tex.move_to(\n",
    "            self.camera.frame.get_corner(UL) + DOWN * 0.5 + RIGHT * 0.5, aligned_edge=UL\n",
    "        )\n",
    "\n",
    "        query_tex = MarkupText(\n",
    "            text,\n",
    "            font_size=font_sizes[0],\n",
    "        )\n",
    "        if not move_camera:\n",
    "            query_tex.shift(DOWN * 1)\n",
    "        self.play(\n",
    "            Write(query_tex),\n",
    "            FadeIn(title_tex, shift=UP * 0.5),\n",
    "            run_time=2 / speed,\n",
    "        )\n",
    "\n",
    "        tokenize_arrow = Arrow(\n",
    "            query_tex.get_bottom(),\n",
    "            query_tex.get_bottom() + DOWN * 1.5,\n",
    "            buff=0.2,\n",
    "        )\n",
    "        anims = [GrowArrow(tokenize_arrow)]\n",
    "        if move_camera:\n",
    "            anims.append(self.camera.frame.animate.move_to(tokenize_arrow.get_end()))\n",
    "        self.play(\n",
    "            *anims,\n",
    "            run_time=1 / speed,\n",
    "        )\n",
    "\n",
    "        query_tokenized_tex = VGroup()\n",
    "        for i, token in enumerate(tokenized_text):\n",
    "            if token == \"<|endoftext|>\":\n",
    "                # Escape properly\n",
    "                token_tex = Tex(\n",
    "                    r\"\\texttt{<|endoftext|>}\",\n",
    "                    font_size=int(font_sizes[1] * 0.8),\n",
    "                )\n",
    "            else:\n",
    "                token_tex = Tex(\n",
    "                    token.strip(),\n",
    "                    font_size=font_sizes[1],\n",
    "                )\n",
    "            token_tex.add(\n",
    "                SurroundingRectangle(token_tex, color=WHITE, buff=0.1, stroke_width=1)\n",
    "            )\n",
    "            query_tokenized_tex.add(token_tex)\n",
    "        # Consistent height for all SurroundingRectangles\n",
    "        max_height = max(tex.height for tex in query_tokenized_tex)\n",
    "        for tex in query_tokenized_tex:\n",
    "            tex.set_height(max_height)\n",
    "        query_tokenized_tex.arrange(RIGHT, buff=0.1).move_to(\n",
    "            tokenize_arrow.get_end() + DOWN * 0.5\n",
    "        )\n",
    "        self.play(\n",
    "            LaggedStartMap(\n",
    "                Write,\n",
    "                query_tokenized_tex,\n",
    "                lag_ratio=0.1,\n",
    "            ),\n",
    "            run_time=2 / speed,\n",
    "        )\n",
    "\n",
    "        # Now create arrows from each token to the embedding\n",
    "        embedding_arrows = Group()\n",
    "        for token_tex in query_tokenized_tex:\n",
    "            arrow = Arrow(\n",
    "                token_tex.get_bottom(),\n",
    "                token_tex.get_bottom() + DOWN * 1,\n",
    "                buff=0.2,\n",
    "            )\n",
    "            embedding_arrows.add(arrow)\n",
    "\n",
    "        anims = [GrowArrow(arrow) for arrow in embedding_arrows]\n",
    "        if move_camera:\n",
    "            anims.append(\n",
    "                self.camera.frame.animate.move_to(\n",
    "                    [\n",
    "                        self.camera.frame.get_x(),\n",
    "                        embedding_arrows[0].get_end()[1],\n",
    "                        self.camera.frame.get_z(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        self.play(\n",
    "            *anims,\n",
    "            run_time=1 / speed,\n",
    "        )\n",
    "        # Now show the embeddings\n",
    "        query_embedding_tex = Group()\n",
    "        for i, arrow in enumerate(embedding_arrows):\n",
    "            embedding_value = embedding[i]\n",
    "            embedding_vector = VGroup()\n",
    "            for val in embedding_value[: self.shown_embeddings]:\n",
    "                val_tex = DecimalNumber(\n",
    "                    float(val),\n",
    "                    num_decimal_places=2,\n",
    "                    font_size=font_sizes[2],\n",
    "                    include_sign=True,\n",
    "                )\n",
    "                embedding_vector.add(val_tex)\n",
    "            embedding_vector.add(MathTex(r\"\\vdots\", font_size=font_sizes[2]))\n",
    "            embedding_vector.arrange(DOWN, buff=0.1)\n",
    "            embedding_vector.next_to(arrow.get_end(), DOWN, buff=0.1)\n",
    "            query_embedding_tex.add(embedding_vector)\n",
    "        self.play(\n",
    "            LaggedStartMap(\n",
    "                Write,\n",
    "                query_embedding_tex,\n",
    "                lag_ratio=0.1,\n",
    "            ),\n",
    "            run_time=2 / speed,\n",
    "        )\n",
    "\n",
    "        # Now replace the numbers for a square of color\n",
    "        query_embedding_squares = Group()\n",
    "        for i, vector in enumerate(query_embedding_tex):\n",
    "            for j, val in enumerate(vector):\n",
    "                if j == len(vector) - 1:\n",
    "                    # Skip the ellipsis\n",
    "                    query_embedding_squares.add(val.copy())\n",
    "                else:\n",
    "                    value = embedding[i][j].item()\n",
    "                    square = Square(\n",
    "                        side_length=val.height + 0.05,\n",
    "                        fill_color=self.get_color_from_cmap(\n",
    "                            (float(value) + 1) / 2,\n",
    "                            cmap_name=\"plasma\",\n",
    "                            cmap_range=(-10, 10),\n",
    "                        ),\n",
    "                        fill_opacity=1,\n",
    "                        stroke_opacity=0,\n",
    "                    )\n",
    "                    square.move_to(val.get_center())\n",
    "                    query_embedding_squares.add(square)\n",
    "        self.play(\n",
    "            FadeOut(query_embedding_tex),\n",
    "            FadeIn(query_embedding_squares),\n",
    "            FadeOut(title_tex),\n",
    "            run_time=1 / speed,\n",
    "        )\n",
    "        return (\n",
    "            query_tokenized_tex,\n",
    "            query_tex,\n",
    "            tokenize_arrow,\n",
    "            embedding_arrows,\n",
    "            query_embedding_squares,\n",
    "        )\n",
    "\n",
    "    def create_table(\n",
    "        self, tokenized_query, tokenized_document, sim_matrix, font_size=36\n",
    "    ):\n",
    "        assert len(tokenized_query) == sim_matrix.shape[0]\n",
    "        assert len(tokenized_document) == sim_matrix.shape[1]\n",
    "        # Create y-axis labels\n",
    "        y_axis_labels = VGroup()\n",
    "        for i, token in enumerate(tokenized_query):\n",
    "            if token == \"<|endoftext|>\":\n",
    "                # Escape properly\n",
    "                token_tex = Tex(\n",
    "                    r\"\\texttt{<|endoftext|>}\",\n",
    "                    font_size=int(font_size * 0.8),\n",
    "                )\n",
    "            else:\n",
    "                token_tex = Tex(\n",
    "                    token.strip(),\n",
    "                    font_size=font_size,\n",
    "                )\n",
    "            y_axis_labels.add(token_tex)\n",
    "\n",
    "        y_axis_labels.arrange(DOWN, buff=0.2, aligned_edge=RIGHT).move_to(LEFT * 3)\n",
    "\n",
    "        # Create x-axis labels\n",
    "        x_axis_labels = VGroup()\n",
    "\n",
    "        for i, token in enumerate(tokenized_document):\n",
    "            x_axis_label = Tex(\n",
    "                token.strip(),\n",
    "                font_size=font_size,\n",
    "            ).rotate(PI / 2)\n",
    "            rect = Rectangle()\n",
    "            rect.move_to(\n",
    "                x_axis_label.get_corner(UP),\n",
    "                aligned_edge=UP,\n",
    "            )\n",
    "            x_axis_label.add(rect)\n",
    "            x_axis_labels.add(x_axis_label)\n",
    "        # Make sure that the heights of the x-axis labels are consistent\n",
    "        max_w = max(tex[0].width for tex in x_axis_labels)\n",
    "        for tex in x_axis_labels:\n",
    "            tex[1].width = max_w\n",
    "\n",
    "        x_axis_labels.arrange(RIGHT, buff=0.2, aligned_edge=UP).next_to(\n",
    "            y_axis_labels.get_corner(DR), DR, buff=0.2, aligned_edge=UL\n",
    "        )\n",
    "\n",
    "        # Create the heatmap\n",
    "        heatmap = [Group() for _ in range(sim_matrix.shape[0])]\n",
    "        val_range = (sim_matrix.min(), sim_matrix.max())\n",
    "        for i in range(sim_matrix.shape[0]):\n",
    "            for j in range(sim_matrix.shape[1]):\n",
    "                value = sim_matrix[i, j]\n",
    "                square = Square(\n",
    "                    side_length=0.4,\n",
    "                    fill_color=self.get_color_from_cmap(\n",
    "                        float(value),\n",
    "                        cmap_name=\"hot\",\n",
    "                        cmap_range=val_range,\n",
    "                    ),\n",
    "                    fill_opacity=1,\n",
    "                    stroke_opacity=0,\n",
    "                )\n",
    "                square.move_to(\n",
    "                    [\n",
    "                        x_axis_labels[j].get_x(),\n",
    "                        y_axis_labels[i].get_y(),\n",
    "                        0,\n",
    "                    ]\n",
    "                )\n",
    "                heatmap[i].add(square)\n",
    "        heatmap = Group(*heatmap)\n",
    "\n",
    "        return y_axis_labels, x_axis_labels, heatmap\n",
    "\n",
    "    def construct(self):\n",
    "        self.renderer.camera.use_z_index = True\n",
    "        # Add watermark\n",
    "        watermark = Tex(\n",
    "            \"Carles Onielfa\",\n",
    "            font_size=30,\n",
    "            color=GRAY,\n",
    "        )\n",
    "        # Set to follow the camera\n",
    "        watermark.add_updater(\n",
    "            lambda m: m.move_to(\n",
    "                self.camera.frame.get_corner(UR) + DOWN * 0.5 + LEFT * 0.5,\n",
    "                aligned_edge=UR,\n",
    "            )\n",
    "        )\n",
    "        self.add(watermark)\n",
    "        # Definitions\n",
    "        query = \"Which flora is found in Yosemite?\"\n",
    "        document = text\n",
    "        with open(\"assets/project_video_scene_embeddings.pkl\", \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        query_embedding = embeddings[\"query_embedding\"]\n",
    "        document_embedding = embeddings[\"document_embedding\"]\n",
    "        tokenized_query = embeddings[\"tokenized_query\"]\n",
    "        tokenized_document = embeddings[\"tokenized_document\"]\n",
    "        sim_matrix = (\n",
    "            query_embedding[:, : self.shown_embeddings]\n",
    "            @ document_embedding[:-1, : self.shown_embeddings].T\n",
    "        )\n",
    "        table_data = (\n",
    "            torch.nn.functional.normalize(sim_matrix.to(torch.float16)).numpy() + 1\n",
    "        )\n",
    "        # 1. tokenizing query and document and generating embeddings. show the embeddings\n",
    "        (\n",
    "            q_tokenized_tex,\n",
    "            q_tex,\n",
    "            q_tokenize_arrow,\n",
    "            q_embedding_arrows,\n",
    "            q_embedding_squares,\n",
    "        ) = self.embed_text(\n",
    "            query,\n",
    "            tokenized_query,\n",
    "            query_embedding,\n",
    "            move_camera=True,\n",
    "            title=\"Query Embedding\",\n",
    "        )\n",
    "        self.wait(1)\n",
    "        # Translate the query_tokenized_tex to the bottom of the screen while fading out other elements\n",
    "        self.play(\n",
    "            q_tokenized_tex.animate.move_to(\n",
    "                self.camera.frame.get_bottom() + UP\n",
    "            ),  # Move to the bottom of the screen\n",
    "            FadeOut(q_tex),\n",
    "            FadeOut(q_tokenize_arrow),\n",
    "            FadeOut(q_embedding_arrows),\n",
    "            FadeOut(q_embedding_squares),\n",
    "            run_time=1,\n",
    "        )\n",
    "        (\n",
    "            d_tokenized_tex,\n",
    "            d_tex,\n",
    "            d_tokenize_arrow,\n",
    "            d_embedding_arrows,\n",
    "            d_embedding_squares,\n",
    "        ) = self.embed_text(\n",
    "            document,\n",
    "            tokenized_document,\n",
    "            document_embedding,\n",
    "            move_camera=False,\n",
    "            title=\"Document Embedding\",\n",
    "            font_sizes=(30, 18, 16),\n",
    "            speed=1.75,\n",
    "        )\n",
    "        self.wait(1)\n",
    "        self.play(\n",
    "            FadeOut(d_tex),\n",
    "            FadeOut(d_tokenize_arrow),\n",
    "            FadeOut(d_embedding_arrows),\n",
    "            FadeOut(d_embedding_squares),\n",
    "        )\n",
    "        # 2. rotate the tokenized query 90% to arrange in a matrix, display the heatmap of relevance between query and document\n",
    "        y_axis_labels, x_axis_labels, heatmap = self.create_table(\n",
    "            tokenized_query, tokenized_document[:-1], table_data\n",
    "        )\n",
    "        table_group = Group(\n",
    "            y_axis_labels,\n",
    "            x_axis_labels,\n",
    "            heatmap,\n",
    "        )\n",
    "        table_group.move_to(self.camera.frame.get_center() + DOWN * 0.8 + LEFT * 0.5)\n",
    "        # Fade out the other elements\n",
    "        anims = []\n",
    "        # Transform the query tokenized text to the y-axis labels\n",
    "        anims.extend(\n",
    "            [\n",
    "                Transform(\n",
    "                    q_tokenized_tex[i][0],\n",
    "                    y_axis_labels[i],\n",
    "                )\n",
    "                for i in range(len(q_tokenized_tex))\n",
    "            ]\n",
    "            + [FadeOut(q[1]) for q in q_tokenized_tex]\n",
    "        )\n",
    "        # Transform the document tokenized text to the x-axis labels\n",
    "        anims.extend(\n",
    "            [d.animate.rotate(PI / 2) for d in d_tokenized_tex[:-1]]\n",
    "            + [\n",
    "                Transform(\n",
    "                    d_tokenized_tex[i][0],\n",
    "                    x_axis_labels[i][0],\n",
    "                    replace_mobject_with_target_in_scene=True,\n",
    "                )\n",
    "                if i < len(d_tokenized_tex) - 1\n",
    "                else FadeOut(d_tokenized_tex[i])\n",
    "                for i in range(len(d_tokenized_tex))\n",
    "            ]\n",
    "            + [FadeOut(d[1]) for d in d_tokenized_tex[:-1]]\n",
    "        )\n",
    "        self.play(\n",
    "            *anims,\n",
    "            run_time=1,\n",
    "        )\n",
    "        self.wait(1)\n",
    "        heatmap_title = MarkupText(\n",
    "            \"Compute Token-level Relevance\",\n",
    "            font_size=36,\n",
    "            color=WHITE,\n",
    "        )\n",
    "        heatmap_title.move_to(\n",
    "            self.camera.frame.get_corner(UL) + DOWN * 0.5 + RIGHT * 0.5, aligned_edge=UL\n",
    "        )\n",
    "        self.play(\n",
    "            LaggedStartMap(\n",
    "                FadeIn,\n",
    "                heatmap,\n",
    "                lag_ratio=0.1,\n",
    "            ),\n",
    "            FadeIn(heatmap_title, shift=UP * 0.5),\n",
    "            run_time=2,\n",
    "        )\n",
    "        self.wait(1)\n",
    "        # 3. flatten to show only the heatmap from the <|endoftext|> token to the document tokens\n",
    "\n",
    "        # Fade out the all but the last row of the heatmap and the y-axis labels\n",
    "        flat_heatmap_title = MarkupText(\n",
    "            \"Focus on the similarities for the last query token\",\n",
    "            font_size=36,\n",
    "            color=WHITE,\n",
    "        )\n",
    "        flat_heatmap_title.move_to(\n",
    "            self.camera.frame.get_corner(UL) + DOWN * 0.5 + RIGHT * 0.5, aligned_edge=UL\n",
    "        )\n",
    "        self.play(\n",
    "            FadeOut(heatmap_title),\n",
    "            FadeIn(flat_heatmap_title, shift=UP * 0.5),\n",
    "            LaggedStartMap(\n",
    "                FadeOut,\n",
    "                heatmap[:-1],\n",
    "                lag_ratio=0.1,\n",
    "            ),\n",
    "            LaggedStartMap(\n",
    "                FadeOut,\n",
    "                [q[0] for q in q_tokenized_tex[:-1]],\n",
    "                lag_ratio=0.1,\n",
    "            ),\n",
    "            run_time=1,\n",
    "        )\n",
    "\n",
    "        # 4. animate to show a line plot from the heatmap\n",
    "\n",
    "        y_values = table_data[-1]  # Last row of the heatmap\n",
    "        x_values = np.arange(len(y_values)) + 1\n",
    "        ax = Axes(\n",
    "            x_range=(0, len(y_values) + 1),\n",
    "            y_range=(0, y_values.max() + 0.4),\n",
    "            y_length=self.camera.frame.get_height() * 0.65,\n",
    "        )\n",
    "        ax.set_z_index(1)\n",
    "        # Ax bottom left corner to y_axis labels bottom right corner\n",
    "        ax.width = x_axis_labels.width + 0.75\n",
    "        ax.move_to(y_axis_labels.get_corner(DR) + LEFT * 0.22, aligned_edge=DL)\n",
    "\n",
    "        self.play(\n",
    "            FadeOut(q_tokenized_tex[-1][0]),\n",
    "            Create(ax.x_axis),\n",
    "            Create(ax.y_axis),\n",
    "        )\n",
    "        lines = VGroup()\n",
    "        for i in range(len(x_values) - 1):\n",
    "            start = ax.c2p(x_values[i], y_values[i])\n",
    "            end = ax.c2p(x_values[i + 1], y_values[i + 1])\n",
    "            l = Line(start, end)\n",
    "            lines.add(l)\n",
    "            l.set_z_index(0)\n",
    "        lines.set_z_index(0)\n",
    "\n",
    "        # Plot the points\n",
    "        dots = VGroup()\n",
    "        for x, y in zip(x_values, y_values):\n",
    "            dot = Dot(\n",
    "                radius=0.08,\n",
    "                point=ax.c2p(x, y),\n",
    "                stroke_width=2,\n",
    "                stroke_color=WHITE,\n",
    "                color=self.get_color_from_cmap(\n",
    "                    y,\n",
    "                    cmap_name=\"hot\",\n",
    "                    cmap_range=(table_data.min(), table_data.max()),\n",
    "                ),\n",
    "            )\n",
    "            dots.add(dot)\n",
    "            dot.set_z_index(2)\n",
    "        dots.set_z_index(2)\n",
    "\n",
    "        # Transform the heatmap squares into dots\n",
    "        self.play(\n",
    "            *[\n",
    "                Transform(\n",
    "                    heatmap[-1][i][0],\n",
    "                    dots[i],\n",
    "                    replace_mobject_with_target_in_scene=True,\n",
    "                )\n",
    "                for i in range(len(dots))\n",
    "            ],\n",
    "            run_time=2,\n",
    "            lag_ratio=0.1,\n",
    "        )\n",
    "\n",
    "        self.play(\n",
    "            Create(lines),\n",
    "        )\n",
    "        # 5. highlight peaks in the line plot and group them into clusters\n",
    "        peaks_title = MarkupText(\n",
    "            \"Cluster peaks in relevance\",\n",
    "            font_size=36,\n",
    "            color=WHITE,\n",
    "        )\n",
    "        peaks_title.move_to(\n",
    "            self.camera.frame.get_corner(UL) + DOWN * 0.5 + RIGHT * 0.5, aligned_edge=UL\n",
    "        )\n",
    "        self.play(\n",
    "            FadeOut(flat_heatmap_title),\n",
    "            FadeIn(peaks_title, shift=UP * 0.5),\n",
    "            run_time=1,\n",
    "        )\n",
    "        # Draw threshold line\n",
    "        threshold = y_values.max() * 0.893\n",
    "        threshold_line = ax.get_horizontal_line(\n",
    "            ax.c2p(x_values[-1], threshold), color=YELLOW, stroke_width=3\n",
    "        )\n",
    "        self.play(\n",
    "            Create(threshold_line), *[dot.animate.set_color(WHITE) for dot in dots]\n",
    "        )\n",
    "        self.wait(1.5)\n",
    "        # Flash the points above the threshold\n",
    "        anims = []\n",
    "        peaks: list[Dot] = []\n",
    "        for i, y in enumerate(y_values):\n",
    "            if y > threshold:\n",
    "                peaks.append(dots[i])\n",
    "                anims.append(\n",
    "                    Flash(\n",
    "                        dots[i],\n",
    "                        color=RED,\n",
    "                    )\n",
    "                )\n",
    "                anims.append(dots[i].animate.set_color(RED).scale(1.5))\n",
    "        self.play(LaggedStart(*anims, lag_ratio=0.2))\n",
    "        # Create clusters by scanning the line plot\n",
    "        scan = Rectangle(\n",
    "            width=0.01,\n",
    "            height=(ax.c2p(0, y_values.max() + 0.1) - ax.c2p(0, 0))[1],\n",
    "            color=RED,\n",
    "            stroke_width=2,\n",
    "            fill_opacity=0.1,\n",
    "        )\n",
    "        scan.z_index = 0\n",
    "        scan.move_to(ax.c2p(0, 0), aligned_edge=DOWN)\n",
    "        # Animate the scan\n",
    "        self.play(\n",
    "            Create(scan),\n",
    "            run_time=0.5,\n",
    "        )\n",
    "        destination = np.array(\n",
    "            [\n",
    "                peaks[0].get_x(),\n",
    "                scan.get_y(),\n",
    "                0,\n",
    "            ]\n",
    "        )\n",
    "        distance = np.linalg.norm(destination - scan.get_center())\n",
    "        speed = 2.5\n",
    "        self.play(\n",
    "            scan.animate.move_to(destination),\n",
    "            run_time=distance / speed,\n",
    "            rate_func=linear,\n",
    "        )\n",
    "        target_width = peaks[-1].get_x() - peaks[0].get_x()\n",
    "\n",
    "        self.play(\n",
    "            scan.animate.stretch_to_fit_width(target_width)\n",
    "            .move_to(destination, aligned_edge=LEFT)\n",
    "            .set_run_time(target_width / speed)\n",
    "            .set_rate_func(linear),\n",
    "            # Run these in sequence\n",
    "            LaggedStart(\n",
    "                *[\n",
    "                    label[0].animate.set_color(RED)\n",
    "                    for label in x_axis_labels\n",
    "                    if label[0].get_corner(UL)[0] >= peaks[0].get_x() - 0.5\n",
    "                    and label[0].get_corner(UR)[0] <= peaks[-1].get_x() + 0.5\n",
    "                ],\n",
    "                lag_ratio=0.05,\n",
    "                run_time=target_width / speed,\n",
    "            ),\n",
    "        )\n",
    "        # 6. extract highlighted text\n",
    "        extracted_text_mock = MarkupText(document, font_size=32, color=WHITE)\n",
    "        extracted_text_mock.move_to(self.camera.frame.get_center())\n",
    "        cumm_text_length = [0] + list(\n",
    "            itertools.accumulate([len(t.strip()) for t in tokenized_document[:-1]])\n",
    "        )\n",
    "        self.play(\n",
    "            FadeOut(peaks_title),\n",
    "            FadeOut(ax),\n",
    "            FadeOut(dots),\n",
    "            FadeOut(lines),\n",
    "            FadeOut(threshold_line),\n",
    "            FadeOut(scan),\n",
    "            *[\n",
    "                x_axis_labels[i][0]\n",
    "                .animate.rotate(-PI / 2)\n",
    "                .move_to(\n",
    "                    extracted_text_mock[cumm_text_length[i] : cumm_text_length[i + 1]]\n",
    "                )\n",
    "                for i in range(len(x_axis_labels))\n",
    "            ],\n",
    "        )\n",
    "        # Expand colored text to nearest sentence\n",
    "        start_idx = 6\n",
    "        end_idx = len(tokenized_document) - 1\n",
    "        extend_title = MarkupText(\n",
    "            \"Extend spans to sentences\",\n",
    "            font_size=36,\n",
    "            color=WHITE,\n",
    "        )\n",
    "        extend_title.move_to(\n",
    "            self.camera.frame.get_corner(UL) + DOWN * 0.5 + RIGHT * 0.5, aligned_edge=UL\n",
    "        )\n",
    "        anims_start = []\n",
    "        tokens_start = [\n",
    "            label[0]\n",
    "            for label in x_axis_labels[start_idx:]\n",
    "            if label[0].get_corner(UR)[0] <= peaks[-1].get_x() + 0.5\n",
    "        ]\n",
    "        # Animate each letter separately\n",
    "        for tok in tokens_start:\n",
    "            for letter in tok:\n",
    "                anims_start.append(letter.animate.set_color(RED))\n",
    "        anims_end = []\n",
    "        tokens_end = [\n",
    "            label[0]\n",
    "            for label in x_axis_labels[:end_idx]\n",
    "            if label[0].get_corner(UL)[0] >= peaks[0].get_x() - 0.5\n",
    "        ]\n",
    "        for tok in tokens_end:\n",
    "            for letter in tok:\n",
    "                anims_end.append(letter.animate.set_color(RED))\n",
    "        self.play(\n",
    "            FadeIn(extend_title, shift=UP * 0.5),\n",
    "            LaggedStart(\n",
    "                *reversed(anims_start),\n",
    "                lag_ratio=0.05,\n",
    "            ),\n",
    "            LaggedStart(\n",
    "                *anims_end,\n",
    "                lag_ratio=0.05,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        q_text = MarkupText(query, font_size=54, color=WHITE)\n",
    "        q_text.move_to(self.camera.frame.get_top() + UP * 0.5)\n",
    "        self.play(\n",
    "            FadeOut(extend_title),\n",
    "            q_text.animate.move_to(self.camera.frame.get_top() + DOWN * 2),\n",
    "        )\n",
    "        self.wait(3)\n",
    "        self.play(\n",
    "            FadeOut(q_text),\n",
    "            *[FadeOut(x_axis_labels[i][0]) for i in range(len(x_axis_labels))],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70a4e390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.19.0</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m19.0\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104190/1928968499.py:87: DeprecationWarning: This method is not guaranteed to stay around. Please prefer setting the attribute normally or with Mobject.set().\n",
      "  tex.set_height(max_height)\n",
      "/tmp/ipykernel_104190/1928968499.py:18: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)\n",
      "/tmp/ipykernel_104190/1928968499.py:436: DeprecationWarning: This method is not guaranteed to stay around. Please prefer getting the attribute normally.              \n",
      "  y_length=self.camera.frame.get_height() * 0.65,\n",
      "                                                                                                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"media/jupyter/ProjectVideo@2025-07-05@11-09-07.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%manim -qh -v ERROR ProjectVideo\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7b022ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "from manim import *\n",
    "\n",
    "\n",
    "class DecoderDiagram(MovingCameraScene):\n",
    "    def construct(self):\n",
    "        self.camera.background_color = None\n",
    "        watermark = Tex(\n",
    "            \"Carles Onielfa\",\n",
    "            font_size=30,\n",
    "            color=GRAY,\n",
    "        )\n",
    "        # Set to follow the camera\n",
    "        watermark.add_updater(\n",
    "            lambda m: m.move_to(\n",
    "                self.camera.frame.get_corner(DR) + UP * 0.5 + LEFT * 0.25,\n",
    "                aligned_edge=UR,\n",
    "            )\n",
    "        )\n",
    "        self.add(watermark)\n",
    "\n",
    "        Text.set_default(font=\"monospace\")\n",
    "        RoundedRectangle.set_default(\n",
    "            corner_radius=0.05,\n",
    "            stroke_width=2,\n",
    "        )\n",
    "        Rectangle.set_default(\n",
    "            stroke_width=2,\n",
    "        )\n",
    "        text = \"Help Patrick ... <|endoftext|>\"\n",
    "\n",
    "        token_box_width = 2\n",
    "        # Reusable embedding_block\n",
    "        num_embedding_blocks = 4\n",
    "        embedding_block = VGroup(\n",
    "            *[\n",
    "                Rectangle(\n",
    "                    width=0.89 * token_box_width / num_embedding_blocks,\n",
    "                    height=0.9 * token_box_width / num_embedding_blocks,\n",
    "                )\n",
    "                for _ in range(num_embedding_blocks)\n",
    "            ]\n",
    "        )\n",
    "        embedding_block.arrange(RIGHT, buff=0)\n",
    "\n",
    "        # ----- INPUT TOKENS BLOCK -----\n",
    "        input_tokens_block = VGroup()\n",
    "        for i, word in enumerate(text.split()):\n",
    "            token = RoundedRectangle(corner_radius=0, width=token_box_width, height=0.5)\n",
    "            label = Text(word, font_size=16)\n",
    "            label.move_to(token.get_center())\n",
    "            token.add(label)\n",
    "            # Add the token embedding and the position encoding\n",
    "            token_embedding = embedding_block.copy()\n",
    "            token_embedding.set_color(ORANGE)\n",
    "            plus_sign = Text(\"+\", font_size=32)\n",
    "            position_encoding = embedding_block.copy()\n",
    "            position_encoding.set_color(TEAL)\n",
    "            up_arrow = Arrow(\n",
    "                token_embedding.get_bottom() * 0.75,\n",
    "                token.get_top() * 0.75,\n",
    "                buff=0,\n",
    "            )\n",
    "            block = VGroup(\n",
    "                token, up_arrow, token_embedding, plus_sign, position_encoding\n",
    "            )\n",
    "            block.arrange(UP, buff=0.1)\n",
    "\n",
    "            # Add the rectangle to the input tokens block\n",
    "            input_tokens_block.add(block)\n",
    "\n",
    "        input_tokens_block.arrange(RIGHT, buff=0.3)\n",
    "        input_tokens_block.to_edge(DOWN)\n",
    "\n",
    "        # ----- DECODER BLOCKS -----\n",
    "        # Create a vertical stack of decoder blocks\n",
    "        # Each block represents a layer in the decoder\n",
    "        decoder_block = VGroup()\n",
    "        num_hidden_layers = 36\n",
    "        for i in range(4):\n",
    "            if i == 2:\n",
    "                # Place dots instead of a block for the third layer\n",
    "                block = RoundedRectangle(\n",
    "                    width=input_tokens_block.width,\n",
    "                    height=0.8,\n",
    "                    stroke_opacity=0,\n",
    "                )\n",
    "                label = Text(f\"...\", font_size=28)\n",
    "                label.move_to(block.get_center())\n",
    "                block.add(label)\n",
    "            else:\n",
    "                block = RoundedRectangle(\n",
    "                    width=input_tokens_block.width, height=0.8, fill_opacity=0.6\n",
    "                )\n",
    "                block.set_fill(BLACK)\n",
    "\n",
    "                block.z_index = 2\n",
    "            block.move_to(ORIGIN + DOWN * i)\n",
    "            decoder_block.add(block)\n",
    "        decoder_block.arrange(DOWN, buff=0.1)\n",
    "\n",
    "        # ----- HIDDEN STATES -----\n",
    "        hidden_states = VGroup()\n",
    "\n",
    "        for block in input_tokens_block:\n",
    "            # hidden rectangle the same size as the input token block for matching width\n",
    "            last_hidden_state = RoundedRectangle(\n",
    "                corner_radius=0.1,\n",
    "                width=token_box_width,\n",
    "                height=0.5,\n",
    "                stroke_width=0,\n",
    "            )\n",
    "            b = embedding_block.copy()\n",
    "            last_hidden_state.add(b)\n",
    "            hidden_states.add(last_hidden_state)\n",
    "\n",
    "        hidden_states.arrange(RIGHT, buff=0.3)\n",
    "\n",
    "        # Add the input tokens block and decoder block to the scene\n",
    "        llm_stack = VGroup(input_tokens_block, decoder_block, hidden_states)\n",
    "        llm_stack.arrange(UP, buff=0.5)\n",
    "        # llm_stack.move_to(llm_stack.get_center() + RIGHT * 1)\n",
    "\n",
    "        # ----- LABELS -----\n",
    "        label_font_size = 22\n",
    "        token = input_tokens_block[0][0]\n",
    "        token_embedding = input_tokens_block[0][2]\n",
    "        position_encoding = input_tokens_block[0][4]\n",
    "\n",
    "        # Token labels\n",
    "        labels_group = Group()\n",
    "        token_label = Tex(f\"Input Tokens\", font_size=label_font_size)\n",
    "        token_label.move_to(token.get_left() + LEFT * 0.15, aligned_edge=RIGHT)\n",
    "        labels_group.add(token_label)\n",
    "\n",
    "        # Token embedding label\n",
    "        token_embedding_label = Tex(\n",
    "            r\"$\\blacksquare$ Token Embeddings\",\n",
    "            font_size=label_font_size,\n",
    "        )\n",
    "        token_embedding_label[0][0].set_color(ORANGE)\n",
    "        token_embedding_label.move_to(\n",
    "            token_embedding.get_left() + LEFT * 0.15, aligned_edge=RIGHT\n",
    "        )\n",
    "        labels_group.add(token_embedding_label)\n",
    "\n",
    "        # Position encoding label\n",
    "        position_encoding_label = Tex(\n",
    "            r\"$\\blacksquare$ Position Encodings\",\n",
    "            font_size=label_font_size,\n",
    "        )\n",
    "        position_encoding_label[0][0].set_color(TEAL)\n",
    "        position_encoding_label.move_to(\n",
    "            position_encoding.get_left() + LEFT * 0.15, aligned_edge=RIGHT\n",
    "        )\n",
    "        labels_group.add(position_encoding_label)\n",
    "\n",
    "        # Decoder block labels\n",
    "        decoder_block_labels = Group()\n",
    "        for i, block in enumerate(decoder_block):\n",
    "            if i != 2:\n",
    "                label = Tex(\n",
    "                    f\"Decoder block {1 if i == 3 else num_hidden_layers - i}\",\n",
    "                    font_size=label_font_size,\n",
    "                )\n",
    "            else:\n",
    "                # Add dummy label for the third block\n",
    "                label = Tex(\"\")\n",
    "\n",
    "            decoder_block_labels.add(label)\n",
    "            label.move_to(block.get_left() + LEFT * 0.15, aligned_edge=RIGHT)\n",
    "\n",
    "        # Default embedding label\n",
    "        default_embedding_label = Tex(\n",
    "            r\"$\\blacksquare$ Text Embedding\\\\(default embedding)\",\n",
    "            font_size=label_font_size,\n",
    "        )\n",
    "        default_embedding_label[0][0].set_color(YELLOW)\n",
    "        default_embedding_label.move_to(\n",
    "            hidden_states.get_right() + RIGHT * 0.4, aligned_edge=LEFT\n",
    "        )\n",
    "        default_embedding_label_arrow = Arrow(\n",
    "            hidden_states.get_right(),\n",
    "            default_embedding_label.get_left() + LEFT * 0.1,\n",
    "            buff=0,\n",
    "        )\n",
    "        # Hidden states label\n",
    "        hidden_states_label = Tex(\n",
    "            r\"$\\blacksquare + \\blacksquare$ Hidden States\\\\(our embeddings)\",\n",
    "            font_size=label_font_size,\n",
    "        )\n",
    "        hidden_states_label[0][0].set_color(PURPLE)\n",
    "        hidden_states_label[0][2].set_color(YELLOW)\n",
    "        hidden_states_label.move_to(\n",
    "            hidden_states.get_left() + LEFT * 0.15, aligned_edge=RIGHT\n",
    "        )\n",
    "        labels_group.add(hidden_states_label)\n",
    "\n",
    "        # ----- OUTPUT ARROWS -----\n",
    "        output_arrows = Group()\n",
    "        for input_block, output_block in zip(input_tokens_block, hidden_states):\n",
    "            # Create an arrow from the input token block to the hidden state block\n",
    "            arrow = Arrow(\n",
    "                input_block.get_top(),\n",
    "                output_block.get_bottom(),\n",
    "                buff=0.1,\n",
    "                stroke_width=2,\n",
    "                max_tip_length_to_length_ratio=0.03,\n",
    "            )\n",
    "            arrow.z_index = 0\n",
    "            output_arrows.add(arrow)\n",
    "\n",
    "        # ----- ANIMATION -----\n",
    "        # Show input tokens block\n",
    "        self.play(FadeIn(*[i[0] for i in input_tokens_block]), run_time=1)\n",
    "        self.play(Create(token_label, run_time=0.5))\n",
    "        self.wait(1)\n",
    "        # Show decoder blocks\n",
    "        # self.play(\n",
    "        #     FadeIn(*reversed(decoder_block), lag_ratio=0.2),\n",
    "        #     run_time=1,\n",
    "        # )\n",
    "        # self.play(\n",
    "        #     *[Create(label) for label in reversed(decoder_block_labels)],\n",
    "        #     run_time=1,\n",
    "        #     lag_ratio=0.2,\n",
    "        # )\n",
    "        self.play(\n",
    "            Succession(\n",
    "                *[\n",
    "                    (FadeIn(block), Create(label))\n",
    "                    for block, label in zip(\n",
    "                        reversed(decoder_block), reversed(decoder_block_labels)\n",
    "                    )\n",
    "                ],\n",
    "                lag_ratio=0.2,\n",
    "                run_time=1,\n",
    "            )\n",
    "        )\n",
    "        self.wait(1)\n",
    "        # Show input token embeddings and position encodings\n",
    "        self.play(\n",
    "            FadeIn(*[i[2] for i in input_tokens_block]),\n",
    "            FadeIn(*[i[4] for i in input_tokens_block]),\n",
    "            run_time=1,\n",
    "        )\n",
    "        # Show labels for input token embeddings and position encodings\n",
    "\n",
    "        self.play(\n",
    "            Create(token_embedding_label),\n",
    "            Create(position_encoding_label),\n",
    "            run_time=0.5,\n",
    "        )\n",
    "        self.wait(1)\n",
    "        # Show arrows\n",
    "        self.play(\n",
    "            FadeIn(*[i[1] for i in input_tokens_block]),\n",
    "            run_time=0.5,\n",
    "        )\n",
    "        self.play(\n",
    "            FadeIn(*[i[3] for i in input_tokens_block]),\n",
    "            run_time=0.5,\n",
    "        )\n",
    "        self.play(\n",
    "            *[GrowArrow(arrow) for arrow in output_arrows],\n",
    "            run_time=1,\n",
    "        )\n",
    "        self.wait(0.5)\n",
    "        # Show hidden states\n",
    "        self.play(\n",
    "            FadeIn(*hidden_states),\n",
    "            run_time=1,\n",
    "        )\n",
    "        # Show default embedding label and arrow\n",
    "        # while flickering the last embedding block to yellow\n",
    "        last_embedding_block = hidden_states[-1]\n",
    "        self.play(\n",
    "            last_embedding_block.animate.set_color(YELLOW),\n",
    "            GrowArrow(default_embedding_label_arrow),\n",
    "            Create(default_embedding_label),\n",
    "            run_time=1,\n",
    "        )\n",
    "        self.wait(1.5)\n",
    "        self.play(\n",
    "            hidden_states[:-1].animate.set_color(PURPLE),\n",
    "            Create(hidden_states_label),\n",
    "            run_time=0.5,\n",
    "        )\n",
    "\n",
    "        self.wait(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c168b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a021348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.19.0</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m19.0\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"media/jupyter/DecoderDiagram@2025-07-05@11-17-09.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%manim -qh -v WARNING DecoderDiagram\n",
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
